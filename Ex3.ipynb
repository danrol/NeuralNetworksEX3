{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-010f228cec50>:14: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_DATA\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_DATA\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Asus\\.conda\\envs\\vroscopy2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "from datetime import timedelta\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_DATA\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_fscore(networks):\n",
    "    for i in range(len(networks)):\n",
    "        for j in range(len(networks) - 1):\n",
    "            if networks[j].fscore[2] < networks[j + 1].fscore[2]:\n",
    "                networks[j], networks[j + 1] = networks[j + 1], networks[j]\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape, dtype=None, name=\"Const\")\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, w, padding='SAME'):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=padding)\n",
    "\n",
    "\n",
    "def max_pooling_2x2(x, padding='SAME'):\n",
    "    return tf.nn.max_pool2d(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network:\n",
    "    # scores for data will be kept in the following order:\n",
    "    # fscore, accuracy, recall, precision = ['train', 'validation', 'test']\n",
    "    class DataType:\n",
    "        Names = [\"TRAIN\", \"VALIDATION\", \"TEST\"]\n",
    "        TRAIN = 0\n",
    "        VALIDATION = 1\n",
    "        TEST = 2\n",
    "\n",
    "    def __init__(self, session, network_type, batch_size=100, num_iterations=13000, keep_probability_value=0.5):\n",
    "        \"\"\"\n",
    "        :type session: tensorflow session object\n",
    "        :type network_type: network creator function pointer\n",
    "        \"\"\"\n",
    "        self.fscore = [0, 0, 0]\n",
    "        self.accuracy = [0, 0, 0]\n",
    "        self.recall = [0, 0, 0]\n",
    "        self.precision = [0, 0, 0]\n",
    "        self.keep_probability_value = keep_probability_value\n",
    "        self.x_placeholder = None\n",
    "        self.z_variables = None\n",
    "        self.keep_prob_placeholder = None\n",
    "        self.target_placeholder = None\n",
    "        self.session = session\n",
    "        self.batch_size = batch_size\n",
    "        self.num_iterations = num_iterations\n",
    "        self.net_type = network_type\n",
    "        self.net = None\n",
    "        self.train_time = timedelta(seconds=0)\n",
    "        self.num_weights_in_net = None\n",
    "        self.net_parameters = None\n",
    "        self.is_conv_net = False\n",
    "\n",
    "    def run(self):\n",
    "        self.build_train()\n",
    "        self.test_all_scores()\n",
    "\n",
    "    def build_train(self):\n",
    "        self.net = self.net_type()  # create network\n",
    "\n",
    "        self.x_placeholder = self.net[0]\n",
    "        self.z_variables = self.net[1]\n",
    "        self.keep_prob_placeholder = self.net[2]\n",
    "        self.target_placeholder = self.net[3]\n",
    "\n",
    "        self.net_parameters = self.net[4]\n",
    "\n",
    "        self.num_weights_in_net = self.net_parameters[4]\n",
    "        if self.keep_prob_placeholder is not None:\n",
    "            self.is_conv_net = True\n",
    "\n",
    "        self.train_network()\n",
    "        logging.info(str(self))\n",
    "\n",
    "    def score(self, y_values, t_values, data_type_enum, printlog=False):\n",
    "        y_val, t_val = np.argmax(y_values, 1), np.argmax(t_values, 1)\n",
    "\n",
    "        self.accuracy[data_type_enum] = accuracy_score(t_val, y_val)\n",
    "\n",
    "        self.fscore[data_type_enum] = f1_score(t_val, y_val, average=\"macro\")\n",
    "\n",
    "        # self.precision[data_type_enum] = precision_score(t_val, y_val, average=\"macro\")\n",
    "        self.precision[data_type_enum] = precision_score(t_val, y_val, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # self.recall[data_type_enum] = recall_score(t_val, y_val, average=\"macro\")\n",
    "        self.recall[data_type_enum] = recall_score(t_val, y_val, average=\"macro\", zero_division=0)\n",
    "        if printlog:\n",
    "            logging.info(self.scores_str(data_type_enum))\n",
    "\n",
    "    def test_all_scores(self):\n",
    "        logging.debug(\"test_all_scores begin\")\n",
    "\n",
    "        y_values, t_values = self.predict(data=mnist.train, use_batch=True)\n",
    "        self.score(y_values, t_values, Network.DataType.TRAIN)\n",
    "\n",
    "        y_values, t_values = self.predict(data=mnist.validation, use_batch=True)\n",
    "        self.score(y_values, t_values, Network.DataType.VALIDATION)\n",
    "\n",
    "        y_values, t_values = self.predict(data=mnist.test, use_batch=True)\n",
    "        self.score(y_values, t_values, Network.DataType.TEST)\n",
    "        logging.debug(\"test_all_scores end\")\n",
    "\n",
    "    def predict(self, data, use_batch=False):\n",
    "        if use_batch:\n",
    "            batch_x, batch_t = data.next_batch(self.batch_size)\n",
    "        else:\n",
    "            batch_x, batch_t = data.images, data.labels  # use whole database\n",
    "\n",
    "        if self.keep_prob_placeholder is None:  # check if conv net or not\n",
    "            y = self.session.run(self.net[1], feed_dict={self.x_placeholder: batch_x})\n",
    "        else:\n",
    "            y = self.session.run(self.net[1], feed_dict={self.x_placeholder: batch_x, self.keep_prob_placeholder: 1})\n",
    "        return y, batch_t\n",
    "\n",
    "    def scores_str(self, data_type_enum=None):\n",
    "        s = \"\"\n",
    "        if data_type_enum is not None:\n",
    "            if data_type_enum == Network.DataType.TRAIN:\n",
    "                s = s + \"\\n\"\n",
    "            s = s + \" - \" + Network.DataType.Names[data_type_enum] + \": \"\n",
    "            s = s + \"Network Scores\\n\\tAccuracy: \" + str(self.accuracy[data_type_enum]) + \" Fscore: \" + str(self.fscore[data_type_enum]) + \"\\n\\tPrecision: \" + str(self.precision[\n",
    "                                                                                                                                                                       data_type_enum]) \\\n",
    "                + \" recall: \" + str(self.recall[data_type_enum]) + \"\\n\"\n",
    "        else:\n",
    "            s = self.scores_str(Network.DataType.TRAIN)\n",
    "            s = s + (self.scores_str(Network.DataType.VALIDATION))\n",
    "            s = s + (self.scores_str(Network.DataType.TEST))\n",
    "        return s\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"Batch_size=\" + str(self.batch_size) + \" Num training iterations=\" + str(self.num_iterations) + \" dropout_rate=\" + str(self.keep_probability_value)\n",
    "        s = s + \"\\n\\t\\tTraining time is : \" + str(self.train_time)\n",
    "        s = s + \"\\n\\t\\tNumber of weights in net is : \" + str(self.num_weights_in_net)\n",
    "        return s\n",
    "\n",
    "    def predict_image_value(self, image):\n",
    "        z = self.session.run([self.z_variables], {self.x_placeholder: [image], self.keep_prob_placeholder: 1})\n",
    "        return np.argmax(z[0], 1)[0]\n",
    "\n",
    "    def visualize(self, image, layer_num, channel_num, before_activation=False):\n",
    "        if layer_num != 0 and layer_num < 3 and self.is_conv_net:\n",
    "            conv_layer = self.net[layer_num + 4]\n",
    "            if before_activation:\n",
    "                the_layer = conv_layer[0]\n",
    "            else:\n",
    "                the_layer = conv_layer[1]\n",
    "            z, layer = self.session.run([self.z_variables, the_layer], {self.x_placeholder: [image], self.keep_prob_placeholder: 1})\n",
    "            prediction = np.argmax(z, 1)\n",
    "            logging.info(\"Printing layer number \" + str(layer_num) + \" channel number: \" + str(channel_num) + \" for predicted value \" + str(prediction) + ( \"before relu \" if before_activation else \" after relu\"))\n",
    "            num_filters = conv_layer[4]\n",
    "            x_size = conv_layer[2]\n",
    "            y_size = conv_layer[3]\n",
    "            first_image = np.transpose(np.array(layer[0], dtype='float').reshape((x_size * y_size, num_filters)))  # 28x28, filter size\n",
    "            # change this according to picture size and filter size in layer\n",
    "            pixels = first_image[channel_num].reshape((x_size, y_size))\n",
    "\n",
    "        else:  # print the input\n",
    "            if self.is_conv_net is True:\n",
    "                logging.info(\"Invalid input, printing the image as is\")\n",
    "            elif self.is_conv_net is False and layer_num != 0:\n",
    "                 logging.info(\"Not a convolution network, printing input\")\n",
    "            else:\n",
    "                logging.info(\"Printing layer 0 which is the input\")\n",
    "            first_image = np.array(image, dtype='float')  # 28x28, filter size\n",
    "            x_size = self.net_parameters[0]\n",
    "            y_size = self.net_parameters[1]\n",
    "            pixels = first_image.reshape((x_size, y_size))\n",
    "\n",
    "        plt.imshow(pixels, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    def train_network(self):\n",
    "        iteration_number_for_target_accuracy = None\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.target_placeholder, logits=self.z_variables))\n",
    "        train_step = tf.compat.v1.train.AdamOptimizer(name=\"Adam\").minimize(cross_entropy)\n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "        t1 = timer()\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            batch_xsx, batch_ts = mnist.train.next_batch(batch_size=self.batch_size)\n",
    "            if self.keep_prob_placeholder is None:\n",
    "                ts, ce = self.session.run([train_step, cross_entropy], feed_dict={self.x_placeholder: batch_xsx, self.target_placeholder: batch_ts})\n",
    "            else:\n",
    "                ts, ce = self.session.run([train_step, cross_entropy],\n",
    "                                          feed_dict={self.x_placeholder: batch_xsx, self.target_placeholder: batch_ts, self.keep_prob_placeholder: self.keep_probability_value})\n",
    "\n",
    "            if iteration_number_for_target_accuracy is None:\n",
    "                y_values, t_values = self.predict(data=mnist.validation, use_batch=True)\n",
    "                self.score(y_values, t_values, Network.DataType.VALIDATION)\n",
    "                if self.accuracy[Network.DataType.VALIDATION] >= 0.99:\n",
    "                    iteration_number_for_target_accuracy = _\n",
    "                    t3 = timer()\n",
    "                    \n",
    "        if iteration_number_for_target_accuracy is not None:\n",
    "            logging.info(\"Reached 99% accuracy within \" + str(iteration_number_for_target_accuracy) + \" iterations and \" + str(timedelta(seconds=t3 - t1)))\n",
    "        t2 = timer()\n",
    "        self.train_time = timedelta(seconds=t2 - t1)\n",
    "        logging.info(\"Training time is : \" + str(self.train_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression_with_layer(x_input_size=28, y_input_size=28, n_output=10, n_hidden1=200, n_hidden2=200):\n",
    "    logging.info(\"***********LOGISTIC REGRESSION WITH LAYER***********\")\n",
    "    logging.info(\"Layer 1 size = \" + str(n_hidden1) + \" Layer 2 size = \" + str(n_hidden2))\n",
    "    n_input = x_input_size * y_input_size\n",
    "    seed = tf.compat.v1.set_random_seed(random.randint(1, 1000))\n",
    "    x = tf.compat.v1.placeholder(tf.float32, [None, n_input], name=\"Inputs\")\n",
    "    t = tf.compat.v1.placeholder(tf.float32, [None, n_output], name=\"Targets\")\n",
    "    h1 = tf.Variable(tf.random.uniform([n_input, n_hidden1], -1, 1, seed=seed), name=\"h1\")\n",
    "    b1 = tf.Variable(tf.random.uniform([1, n_hidden1], -1, 1, seed=seed), name=\"b1\")\n",
    "    h2 = tf.Variable(tf.random.uniform([n_hidden1, n_hidden2], -1, 1, seed=seed), name=\"h2\")\n",
    "    b2 = tf.Variable(tf.random.uniform([1, n_hidden2], -1, 1, seed=seed), name=\"b2\")\n",
    "    w = tf.Variable(tf.random.uniform([n_hidden2, n_output], -1, 1, seed=seed), name=\"Out_layer_w\")\n",
    "    b = tf.Variable(tf.random.uniform([1, n_output], -1, 1, seed=seed), name=\"Out_biases\")\n",
    "\n",
    "    h1_s = tf.add(tf.matmul(x, h1), b1)\n",
    "    h1_s_rel = tf.nn.relu(h1_s)\n",
    "    h2_s = tf.add(tf.matmul(h1_s_rel, h2), b2)\n",
    "    h2_s_rel = tf.nn.relu(h2_s)\n",
    "    z = tf.add(tf.matmul(h2_s_rel, w), b)\n",
    "    num_weights_in_net = n_input * n_hidden1 + n_hidden1 + n_hidden1 * n_hidden2 + n_hidden2 + n_hidden2 * n_output + n_output\n",
    "    net_parameters = [x_input_size, y_input_size, n_output, [n_hidden1, n_hidden2], num_weights_in_net]\n",
    "    return [x, z, None, t, net_parameters, h1_s, h1_s_rel, h2_s, h2_s_rel]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_input_size=28, y_input_size=28, n_output=10):\n",
    "    n_input = x_input_size * y_input_size\n",
    "    num_weights_in_net = n_input * n_output + n_output\n",
    "    logging.info(\"***********LOGISTIC REGRESSION***********\")\n",
    "    logging.info(\"input size = \" + str(n_input) + \" output size = \" + str(n_output))\n",
    "    seed = tf.compat.v1.set_random_seed(random.randint(1, 1000))\n",
    "    x = tf.compat.v1.placeholder(tf.float32, [None, n_input], name=\"Inputs\")\n",
    "    t = tf.compat.v1.placeholder(tf.float32, [None, n_output], name=\"Targets\")\n",
    "    w = tf.Variable(tf.random.uniform([n_input, n_output], -1, 1, seed=seed), name=\"Out_layer_w\")\n",
    "    b = tf.Variable(tf.random.uniform([n_output], -1, 1, seed=seed), name=\"Out_biases\")\n",
    "    z = tf.add(tf.matmul(x, w), b)\n",
    "\n",
    "    net_parameters = [x_input_size, y_input_size, n_output, [], num_weights_in_net]\n",
    "    return [x, z, None, t, net_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_conv_layers(x_input_size=28, y_input_size=28, n_output=10, num_filters1=32, num_filters2=64, drop_rate_percent=0.5, x_filter_size=5,\n",
    "                                    y_filter_size=5, dimension_size=1, hidden_layer_size=1024):\n",
    "    logging.info(\"***********LOGISTIC REGRESSION WITH CONVOLUTION***********\")\n",
    "    x_size_after_pool1 = int(x_input_size / 2)\n",
    "    y_size_after_pool1 = int(y_input_size / 2)\n",
    "    x_size_after_pool2 = int(x_input_size / 4)\n",
    "    y_size_after_pool2 = int(y_input_size / 4)\n",
    "    # first activation map\n",
    "    # input size = N = 786\n",
    "    # stride = S = 1\n",
    "    # padding = P = 8\n",
    "    # filter size = F = 25\n",
    "    # num weights = (N+2P-F)/S+1\n",
    "    num_weights_in_net = (((x_input_size*y_input_size + 2*8 - x_filter_size*y_filter_size)/1) + 1)*num_filters1\n",
    "    num_weights_in_net += (((x_size_after_pool1*y_size_after_pool1 + 2*2 - x_filter_size*y_filter_size)/1) + 1)*num_filters2\n",
    "    num_weights_in_net += x_size_after_pool2*y_size_after_pool2*num_filters2*hidden_layer_size + hidden_layer_size\n",
    "    num_weights_in_net += hidden_layer_size*n_output + n_output\n",
    "    \n",
    "    x = tf.compat.v1.placeholder(tf.float32, [None, x_input_size * y_input_size])\n",
    "    t = tf.compat.v1.placeholder(tf.float32, [None, n_output], name=\"Targets\")\n",
    "    w_conv1 = weight_variable([x_filter_size, y_filter_size, dimension_size, num_filters1])  # 32 filters of 5x5x1 (x axis, y axis, dimension (greyscale is 1))\n",
    "    b_conv1 = bias_variable([num_filters1])\n",
    "\n",
    "    w_conv2 = weight_variable([x_filter_size, y_filter_size, num_filters1, num_filters2])\n",
    "    b_conv2 = bias_variable([num_filters2])\n",
    "\n",
    "    x_image = tf.reshape(x, [-1, x_input_size, y_input_size, 1])\n",
    "    h_conv1 = conv2d(x_image, w_conv1) + b_conv1\n",
    "    h_conv1_relu = tf.nn.relu(h_conv1)\n",
    "    h_pool1 = max_pooling_2x2(h_conv1_relu)\n",
    "    h_conv2 = conv2d(h_pool1, w_conv2) + b_conv2\n",
    "    h_conv2_relu = tf.nn.relu(h_conv2)\n",
    "    h_pool2 = max_pooling_2x2(h_conv2_relu)\n",
    "\n",
    "    # Fully connected layer 1024\n",
    "    w_fc1 = weight_variable([x_size_after_pool2 * y_size_after_pool2 * num_filters2, hidden_layer_size])\n",
    "    b_fc1 = bias_variable([hidden_layer_size])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, x_size_after_pool2 * y_size_after_pool2 * num_filters2])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32)\n",
    "    rate = 1 - keep_prob\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, rate=rate)\n",
    "\n",
    "    w_fc2 = weight_variable([hidden_layer_size, n_output])\n",
    "    b_fc2 = bias_variable([n_output])\n",
    "    y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n",
    "\n",
    "    tf.compat.v1.global_variables_initializer().run()\n",
    "    #   [x_size, y_size, output_size, ['hidden layer size'...], num_weights_in_net]\n",
    "    net_parameters = [x_input_size, y_input_size, n_output, [hidden_layer_size], num_weights_in_net]\n",
    "    return [x, y_conv, keep_prob, t, net_parameters, [h_conv1, h_conv1_relu, x_input_size, y_input_size, num_filters1],\n",
    "            [h_conv2, h_conv2_relu, x_size_after_pool1, y_size_after_pool1, num_filters2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_networks(session):\n",
    "    networks = []\n",
    "    # the following for loops set the network type and other parameters\n",
    "    for batch_size in [50, 100]:\n",
    "        for train_range in [13000]:\n",
    "            net = Network(session=session, network_type=logistic_regression, batch_size=batch_size, num_iterations=train_range)\n",
    "            networks.append(net)\n",
    "            net = Network(session=session, network_type=logistic_regression_with_layer, batch_size=batch_size, num_iterations=train_range)\n",
    "            networks.append(net)\n",
    "            for dropout in [0.5]:\n",
    "                net = Network(session=session, network_type=logistic_regression_conv_layers, batch_size=batch_size, num_iterations=train_range, keep_probability_value=dropout)\n",
    "                networks.append(net)\n",
    "    return networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:No GPU found\n",
      "INFO:root:***********LOGISTIC REGRESSION***********\n",
      "INFO:root:input size = 784 output size = 10\n",
      "INFO:root:Reached 99% accuracy within 3352 iterations and 0:00:30.130757\n",
      "INFO:root:Training time is : 0:00:48.669964\n",
      "INFO:root:Batch_size=50 Num training iterations=13000 dropout_rate=0.5\n",
      "\t\tTraining time is : 0:00:48.669964\n",
      "\t\tNumber of weights in net is : 7850\n",
      "INFO:root:\n",
      " - TRAIN: Network Scores\n",
      "\tAccuracy: 0.94 Fscore: 0.945854145854146\n",
      "\tPrecision: 0.9523809523809523 recall: 0.9464285714285715\n",
      " - VALIDATION: Network Scores\n",
      "\tAccuracy: 0.96 Fscore: 0.9564213564213564\n",
      "\tPrecision: 0.9633333333333333 recall: 0.9583333333333334\n",
      " - TEST: Network Scores\n",
      "\tAccuracy: 0.88 Fscore: 0.8441233766233767\n",
      "\tPrecision: 0.8666666666666666 recall: 0.8541666666666667\n",
      "\n",
      "INFO:root:***********LOGISTIC REGRESSION WITH LAYER***********\n",
      "INFO:root:Layer 1 size = 200 Layer 2 size = 200\n",
      "INFO:root:Reached 99% accuracy within 1215 iterations and 0:00:13.457590\n",
      "INFO:root:Training time is : 0:01:09.073147\n",
      "INFO:root:Batch_size=50 Num training iterations=13000 dropout_rate=0.5\n",
      "\t\tTraining time is : 0:01:09.073147\n",
      "\t\tNumber of weights in net is : 199210\n",
      "INFO:root:\n",
      " - TRAIN: Network Scores\n",
      "\tAccuracy: 1.0 Fscore: 1.0\n",
      "\tPrecision: 1.0 recall: 1.0\n",
      " - VALIDATION: Network Scores\n",
      "\tAccuracy: 0.96 Fscore: 0.947936507936508\n",
      "\tPrecision: 0.9541666666666666 recall: 0.9550000000000001\n",
      " - TEST: Network Scores\n",
      "\tAccuracy: 0.9 Fscore: 0.8356580510992275\n",
      "\tPrecision: 0.8430555555555556 recall: 0.8389285714285715\n",
      "\n",
      "INFO:root:***********LOGISTIC REGRESSION WITH CONVOLUTION***********\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' #  this command forces to use CPU instead of GPU\n",
    "# logging.basicConfig(level=logging.INFO, filename='logger.log', filemode='w')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "if tf.test.gpu_device_name():\n",
    "    logging.info('GPU found')\n",
    "else:\n",
    "    logging.info(\"No GPU found\")\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "networks = create_networks(session=sess)\n",
    "# begin training networks - run performs buildTrain, predict, score and visualize\n",
    "for network in networks:\n",
    "    network.run()\n",
    "    logging.info(network.scores_str())\n",
    "sort_by_fscore(networks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(300):\n",
    "    val = np.argmax(mnist.test.labels[i])\n",
    "    image = mnist.test.images[i]\n",
    "    if val == 6:\n",
    "        break\n",
    "for net_index in range(5):  # print image for 5 networks with best fscores\n",
    "    print(\"*********************************************************************************\")\n",
    "    for layer_num in [1, 2]:\n",
    "        for channel_num in [0]:\n",
    "            for activation in [True, False]:\n",
    "                if net_index <= (len(networks) - 1):\n",
    "                    logging.info(\"Printing net with fscore value = \" + str(networks[net_index].fscore[2]))\n",
    "                    networks[net_index].visualize(image=image, layer_num=layer_num, channel_num=channel_num, before_activation=activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "# Eden Dupont\n",
    "# Daniel Rolnik"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
